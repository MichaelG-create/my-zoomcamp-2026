# dbt Course

## dbt Project Structure
[![](https://markdown-videos-api.jorgenkh.no/youtube/2dYDS4OQbT0)](https://youtu.be/2dYDS4OQbT0)


## analyses
- A place for SQL files that you don't expose (to stakeholders)
- Used for data quality rports
- Lots of people don't use it

## dbt_projects.yml

- The most important file in dbt
- tell dbt some defaults
- need it to run dbt commands
- for dbt core, your profile should match the one in `dbt/profiles.yml`

## macros

- Behave like python functions (reusable logic)
- Help you encapsulate logic (in one place)

## README.md

- The documentation of your project
- isntallation/setup guides
- Contact infos

## seeds

- A space to upload csv dans flat files (to be add to dbt later)
- Quick & dirty approach (better to fix at source)

## snapshots

- takes pictures of the table at a moment in time (example: to keep track of SCD 0 or 1 (better to pu them to SCD 2))
- useful to track the history of a column that overwrites itself

## tests

- A place to put assertions in SQL format (in case the logic breaks, it detects it)
- A place for singular tests
- If this SQL commmand returns more than 0 rows, the dbt build fails.

## models

### staging

- sources (so raw tables form database)
- staging files are 1 to 1 copy of your data with minimal cleaning steps
  - Data types
  - Renaming colums


### intermediate

- anything in between staging (raw) and marts (ready to expose data)
- No guidelines, just nice for heavy duty cleaning or complex logic

### marts

- If it is in marts, it is ready for consumption
- tables ready for dashboards 
- properly modeled, clean tables

### Summary:

<details>
  <summary>Summary of the Video Content on DBT Project Structure and Purpose</summary>

This video provides a detailed explanation of the key files and folders automatically created when initializing a **DBT (Data Build Tool)** project on Mac or Linux environments. It explores the purpose of these files and directories, focusing on how they organize SQL logic, testing, documentation, and temporary data handling within a DBT project.

---

### Key Components and Their Functions

- **Analysis Folder**
  - A place for SQL scripts used for **data quality checks** or **administrative tasks**.
  - Contains scripts not intended to be shared with end stakeholders.
  - Often underutilized but useful for running **data quality reports** rather than tests.

- **DBT Project File (`dbt_project.yml`)**
  - The **most critical file** in a DBT project.
  - Defines the project’s name, profile, default variables, and materializations.
  - Every DBT command checks for this file; absence causes failure.
  - Profile names here must match those in DBT profiles.
  - Supports setting default configurations used across the project.

- **Macros Folder**
  - Contains reusable SQL logic akin to **user-defined functions (UDFs)** or Python functions.
  - Used to encapsulate repeatable and potentially changing logic in one place.
  - Common use cases include:
    - Calendar conversions (e.g., fiscal calendar adjustments).
    - Variable tax rates or business logic.
  - Makes testing smaller chunks of logic easier and more maintainable.

- **Readme Markdown File**
  - Serves as the **project documentation**.
  - Typically includes installation instructions, setup guides, or contact info.
  - Recommended to contain onboarding info such as credential requirements or how to run the project.
  - Auto-generated by DBT but usually customized by teams.

- **Seeds Folder**
  - Used to upload **flat files (e.g., CSVs)** into the database as temporary models.
  - Intended for quick, experimental data ingestion scenarios where:
    - Permanent tables are not yet ready.
    - Permissions are limited.
    - Data changes frequently.
  - Considered a "quick and dirty" approach; ideally, data should be fixed at the source.

- **Snapshots Folder**
  - Used to **track changes over time** in tables where columns are overwritten (e.g., status fields).
  - Records a "picture" of the table at each run, preserving historical changes.
  - Useful for analytical needs when source tables don’t keep history.
  - Best practice is to solve history tracking at the data source if possible.

- **Tests Folder**
  - Contains SQL assertions that act as **data quality tests**.
  - If a test query returns rows, the DBT build fails.
  - Example: Ensuring vehicle timestamps cover exactly 24 hours per day.
  - These "singular tests" help catch logic errors early in the data pipeline.

- **Models Folder (Most Important)**
  - Stores the core SQL logic that transforms raw data into consumable datasets.
  - Typically organized into three subfolders following DBT conventions:
    - **Staging:** One-to-one copies of raw source tables with minimal cleaning (e.g., renaming, filtering nulls).
    - **Intermediate:** Complex transformations, heavy cleaning, joins, or standardization that are not ready for end users.
    - **Marts:** Final, clean data models exposed to end users, dashboards, and BI tools; often star schemas or consumption-ready tables.
  - These guidelines are recommended defaults but can be adapted; alternative naming conventions like "bronze, silver, gold" or others are common.

---

### Core Concepts

| Component       | Purpose                                              | Key Notes                                           |
|-----------------|------------------------------------------------------|----------------------------------------------------|
| Analysis        | Store SQL scripts for internal quality/admin tasks  | Not shared with stakeholders, often underused      |
| dbt_project.yml | Defines project config, profile, defaults            | Mandatory for DBT commands, critical for workflow  |
| Macros          | Encapsulate reusable SQL logic                        | Simplifies updates and testing of repeated logic   |
| Readme          | Project documentation and onboarding information      | Should include instructions for running the project|
| Seeds           | Upload flat files as temporary models                 | Quick experiments, permissions workaround           |
| Snapshots       | Capture historical state of overwriting columns       | Useful for tracking changes, best fixed at source   |
| Tests           | SQL assertions that validate data correctness         | Test failures halt the build, catch logic errors    |
| Models          | Main data transformations                             | Organized into staging, intermediate, marts folders|

---

### Best Practices and Recommendations

- **Maintain `dbt_project.yml` carefully**; ensure profile names align with your environment.
- Use **macros** to centralize frequently changing logic and improve maintainability.
- Keep the **models directory structured** into staging, intermediate, and marts to separate raw data, transformations, and consumption layers.
- Prefer fixing data issues at the source rather than relying heavily on **seeds** or **snapshots**, which serve as temporary or fallback solutions.
- Employ **tests** actively to enforce data quality and catch logic errors early.
- Customize the **readme** to improve project onboarding and usability.

---

### Conclusion

The video thoroughly explains the foundational structure of a DBT project, emphasizing the **importance of configuration (`dbt_project.yml`)**, **modular SQL logic through macros**, and a **clear separation of data transformation stages in models**. It highlights the value of testing and temporary data handling (seeds and snapshots) while encouraging best practices for maintainability and data governance. This overview sets the stage for practical usage and further exploration of DBT commands and SQL file integration.


</details>


